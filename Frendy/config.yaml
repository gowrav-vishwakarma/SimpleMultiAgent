framework:
  base_path: ./
  default_agent: InitialAgent
  pre_prompt: |

  post_prompt: |
    
    Format your response as follows:
    THOUGHTS: [Your reasoning process, including analysis of any tool results]
    USE_TOOL: [Optional: Specify a tool to use, {"tool_name": {"parameter": "value", "parameter": "value"} } ]
    NEXT_AGENT: [AgentName] OR ExecutiveAssistant (Self)
    NEXT_ACTION: FINISH OR CONTINUE
    Begin your response now:

  tool_extract_methods:
    - name: json_format
      regexp: 'USE_TOOL:\s*(\{(?:[^{}]|\{(?:[^{}]|\{[^{}]*\})*\})*\})'
      parse_method: json
      tool_name_extractor: '"(\w+)"'
      params_extractor: ':\s*(\{.*?\})(?=\s*\})'  # Changed this line
    - name: named_with_json
      regexp: 'USE_TOOL:\s*(\w+)\s+with\s+(\{(?:[^{}]|\{(?:[^{}]|\{[^{}]*\})*\})*\})'
      parse_method: json_with_name
      tool_name_extractor: '^(\w+)'
      params_extractor: '(\{.*\})$'
    - name: named_with_key_value
      regexp: 'USE_TOOL:\s*(\w+)\s+with\s+(.+)'
      parse_method: key_value_with_name
      tool_name_extractor: '^(\w+)'
      params_extractor: '(?<=with\s)(.+)$'
  rag:
    enabled: true
    vector_db:
      type: "chromadb"
      path: "./chroma_db"
    embedding_model:
      type: "default"  # ChromaDB uses its own default embedding model
    chunk_size: 1000
    chunk_overlap: 200
    default_retriever:
      search_type: "similarity"
      search_kwargs:
        k: 5
    custom_rag_manager: null  # Set this to the import path of a custom RAG manager class if needed

llm:
  openai:
    api_key: ${OPENAI_API_KEY}
    default_model: gpt-3.5-turbo
  ollama:
    api_base: http://localhost:11434
    default_model: phi3:latest
    stream: true

agents:
  - DeveloperAgent
  - DesignerAgent
  - ProductManagerAgent

tools_path: ./Tools
role_knowledge_path: ./RoleKnowledge

logging:
  level: INFO
  file: framework.log
